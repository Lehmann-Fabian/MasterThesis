{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"~/work/data/test\"\n",
    "topic = \"t21\"\n",
    "full_path = path + \"/\" + topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropFirstXRows(input, x):\n",
    "    return input.drop(np.arange(0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "produced_df = pandas.read_csv(full_path + \"_produced.csv\")\n",
    "start_df = pandas.read_csv(full_path + \".csv\")\n",
    "filtered_df = pandas.read_csv(full_path + \"_filtered.csv\")\n",
    "warnings_df = pandas.read_csv(full_path + \"_warnings.csv\")\n",
    "modelchange_df = pandas.read_csv(full_path + \"_modelchange.csv\")\n",
    "\n",
    "elementsToDrop = 10000\n",
    "produced_df = dropFirstXRows(produced_df,elementsToDrop)\n",
    "start_df = dropFirstXRows(start_df,elementsToDrop)\n",
    "filtered_df = dropFirstXRows(filtered_df,elementsToDrop)\n",
    "\n",
    "#drop changes before first element\n",
    "modelchange_df = modelchange_df[modelchange_df.iloc[:,0] >= produced_df.iloc[0][2]]\n",
    "\n",
    "# drop endtime in warning before first element\n",
    "warnings_df = warnings_df[warnings_df.iloc[:,6] >= produced_df.iloc[0][2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAvgMedStdMinMaxFromArray(diff):\n",
    "    return np.average(diff), np.median(diff), np.std(diff), np.min(diff), np.max(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on the time difference between the arival time of records for all 3 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produced:\n",
      "Got in average every 5.00 ms new data, with median 5.00, and std = 0.53, further min was 0 and max 84\n",
      "\n",
      "\n",
      "produced received:\n",
      "Got in average every 5.00 ms new data, with median 5.00, and std = 0.53, further min was 0 and max 82\n",
      "\n",
      "\n",
      "filtered:\n",
      "Got in average every 5.00 ms new data, with median 5.00, and std = 2.38, further min was 0 and max 893\n",
      "\n",
      "\n",
      "warning:\n",
      "Got in average every 5016.38 ms new data, with median 2493.00, and std = 8391.59, further min was 2 and max 27547\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def avgMedStdArivalTime(df, column):\n",
    "    df = df.to_numpy()[:,column].astype(int)\n",
    "    #print(df)\n",
    "    diff = df[1:,]-df[:-1,]\n",
    "    \n",
    "    return extractAvgMedStdMinMaxFromArray(diff)\n",
    "    \n",
    "def getArivalInfos():\n",
    "    text = \"%s:\\nGot in average every %.2f ms new data, with median %.2f, and std = %.2f, further min was %d and max %d\\n\\n\"\n",
    "    print(text %(\"produced\", *avgMedStdArivalTime(produced_df, 3)))\n",
    "    print(text %(\"produced received\", *avgMedStdArivalTime(start_df, 3)))\n",
    "    print(text %(\"filtered\", *avgMedStdArivalTime(filtered_df, 0)))\n",
    "    print(text %(\"warning\", *avgMedStdArivalTime(warnings_df, 0)))\n",
    "    \n",
    "getArivalInfos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long does it take until a produced record is acknowledged by kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced data was received by Kafka after: avg = 0.00 ms; median 0.00 ms; std = 0.01 ms, further min was 0 ms and max 1 ms\n"
     ]
    }
   ],
   "source": [
    "def kafkaAck(df):\n",
    "    ack = df.to_numpy()[:,1].astype(int)\n",
    "    send = df.to_numpy()[:,3].astype(int)\n",
    "    diff = send - ack\n",
    "    \n",
    "    return extractAvgMedStdMinMaxFromArray(diff)\n",
    "    \n",
    "print(\"Produced data was received by Kafka after: avg = %.2f ms; median %.2f ms; std = %.2f ms, further min was %d ms and max %d ms\" %kafkaAck(produced_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check validity of produced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79119a4d16a459f84f388579674ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid\n"
     ]
    }
   ],
   "source": [
    "def validate(df):\n",
    "    df = df.to_numpy()\n",
    "    lastRow = df[0]\n",
    "    error = False\n",
    "    \n",
    "    l = len(df)\n",
    "    \n",
    "    f = IntProgress(min=0, max=100) # instantiate the bar\n",
    "    display(f)\n",
    "    i = 0\n",
    "    \n",
    "    for x in df[1:]:\n",
    "        \n",
    "        i += 1\n",
    "        cp = int((i / float(l)) * 100)\n",
    "        \n",
    "        if(cp != f.value):\n",
    "            f.value = cp\n",
    "            \n",
    "        if(np.sum(x >= lastRow) != 4):\n",
    "            error = True\n",
    "            print(\"Error:\")\n",
    "            print(lastRow)\n",
    "            print(x)\n",
    "        if(lastRow[0] + 1 != x[0]):\n",
    "            print(\"Offset %i increased not by 1\" %lastRow[0])\n",
    "            \n",
    "        lastRow = x\n",
    "    if(not error):\n",
    "        print(\"Valid\")\n",
    "        \n",
    "validate(produced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loss produced to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected data was received exactly once.\n"
     ]
    }
   ],
   "source": [
    "def calcDataLoss(df1, df1OffsetColumn, df2, df2OffsetColumn):\n",
    "    df1Offsets = df1.to_numpy()[:,df1OffsetColumn].astype(int)\n",
    "    df2Offsets = df2.to_numpy()[:,df2OffsetColumn].astype(int)\n",
    "    \n",
    "    errors = {}\n",
    "        \n",
    "    count = {}\n",
    "    \n",
    "    for x in df1Offsets:\n",
    "        \n",
    "        count[x] = 0\n",
    "\n",
    "        \n",
    "        #count = np.sum(df2Offsets == x)\n",
    "        \n",
    "        #if(count != 1):\n",
    "        #    errors[count] = errors.get(count, 0) + 1\n",
    "        \n",
    "    for x in df1Offsets:\n",
    "        count[x] = count[x] + 1\n",
    "        \n",
    "    for k, v in count.items():\n",
    "        if v != 1:\n",
    "            errors[count] = errors.get(count, 0) + 1\n",
    "    \n",
    "    out = \"Received records \"\n",
    "    losses = False\n",
    "    for k, v in errors.items():\n",
    "        out += \"%ix for %i times; \" %(k,v)\n",
    "        losses = True\n",
    "    \n",
    "    if(losses):\n",
    "        print(out)\n",
    "    else:\n",
    "        print(\"The expected data was received exactly once.\")\n",
    "    \n",
    "calcDataLoss(produced_df, 0, filtered_df, 3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency produced - warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't match all recognized changes! With 0 weren't matched:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "Anomalies were detected by Kafka after: avg = 1703.47 ms; median 1723.00 ms; std = 632.74 ms, further min was 621 ms and max 3132 ms\n"
     ]
    }
   ],
   "source": [
    "def calcLatencyProduceWarning(modelchange_df, warnings_df, amplitude, periodLength):\n",
    "    m = modelchange_df.to_numpy()\n",
    "    w = warnings_df.to_numpy()\n",
    "    lastChange = m[0]\n",
    "\n",
    "    open = False\n",
    "    s = None\n",
    "    \n",
    "    diff = []\n",
    "    \n",
    "    for x in m:\n",
    "        if x[1] == amplitude and x[2] == periodLength:\n",
    "            open = False\n",
    "            \n",
    "            # search warnings\n",
    "            a = w[:,6] >= lastChange[0]\n",
    "\n",
    "            b = w[:,7] <= x[0]\n",
    "            \n",
    "            inner = np.logical_and(a,b)\n",
    "            \n",
    "            leftOuter = np.logical_and(w[:,6] <= lastChange[0], w[:,7] >= lastChange[0])\n",
    "            rightOuter = np.logical_and(w[:,6] <= x[0], w[:,7] >= x[0])\n",
    "\n",
    "            fullRange = np.logical_or(leftOuter, inner)\n",
    "            fullRange = np.logical_or(fullRange, rightOuter)\n",
    "            \n",
    "            if s is None:\n",
    "                fullRange.astype(int)\n",
    "                s = fullRange\n",
    "            else:\n",
    "                s = np.add(s,fullRange.astype(int))\n",
    "            \n",
    "            matching = w[fullRange]     \n",
    "            \n",
    "            #first recognition\n",
    "            r = min(matching[:,0])\n",
    "            diff.append(r - lastChange[0])\n",
    "            \n",
    "        else:\n",
    "            #skip two changes without a reset\n",
    "            if not open:\n",
    "                lastChange = x\n",
    "            open = True\n",
    "        \n",
    "    if not np.all(s == 1):\n",
    "        print(\"Didn't match all recognized changes! With 0 weren't matched:\")\n",
    "        with np.printoptions(threshold=np.inf):\n",
    "            print(s)\n",
    "        \n",
    "    diff = np.array(diff)\n",
    "    \n",
    "    return extractAvgMedStdMinMaxFromArray(diff)\n",
    "\n",
    "print(\"Anomalies were detected by Kafka after: avg = %.2f ms; median %.2f ms; std = %.2f ms, further min was %d ms and max %d ms\" %calcLatencyProduceWarning(modelchange_df, warnings_df, 3, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
