{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "plt.rcParams['figure.figsize'] = [8,5]\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from matplotlib.ticker import StrMethodFormatter, NullFormatter\n",
    "import dictdiffer\n",
    "\n",
    "from tqdm.notebook import tnrange as nrange\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pandas_display_options() -> None:\n",
    "    \"\"\"Set pandas display options.\"\"\"\n",
    "    # Ref: https://stackoverflow.com/a/52432757/\n",
    "    display = pandas.options.display\n",
    "\n",
    "    display.max_columns = 1000\n",
    "    display.max_rows = 100\n",
    "    display.max_colwidth = 199\n",
    "    display.width = None\n",
    "    display.float_format = '{:.2f}'.format\n",
    "    # display.precision = 2  # set as needed\n",
    "set_pandas_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"E:\\\\Studium\\\\10_Semester\\\\Masterarbeit\\\\Deployment\\\\Kafka\\\\results\\\\setup\"\n",
    "output = \"E:\\\\Studium\\\\10_Semester\\\\Masterarbeit\\\\Deployment\\\\Kafka\\\\eval\\\\\"\n",
    "experiment = 1\n",
    "path = path + str(experiment) + os.path.sep\n",
    "output = output + str(experiment) + os.path.sep\n",
    "runs = ['33'] #[x for x in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropFirstXRows(input, x):\n",
    "    return input.drop(np.arange(0,x))\n",
    "\n",
    "def removeNaN(array, array2):\n",
    "    helper = np.logical_not(np.isnan(array))\n",
    "    return array[helper], array2[helper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLastData(generated, received):\n",
    "    lastOffset = received[\"Kafka.Offset\"].to_numpy()[-1]\n",
    "    return generated[generated[\"Kafka.Offset\"] <= lastOffset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDate(line):\n",
    "    return int(datetime.strptime(\" \".join(line.split(\" \")[0:2]),\"%Y-%m-%d %H:%M:%S\").timestamp()) * 1000\n",
    "\n",
    "def transitionsFromFile(path):\n",
    "    f = open(path, \"r\")\n",
    "    start = False\n",
    "    end = False\n",
    "    name = None\n",
    "    last = 0\n",
    "    transitions = []\n",
    "    for line in f.readlines():\n",
    "        if(line.startswith(\"Starting transition to state\")):\n",
    "            name = line.split(\" \")[4][:-1]\n",
    "            start = True\n",
    "        elif(line.startswith(\"Doing state\")):\n",
    "            end = True\n",
    "        elif(start):\n",
    "            last = extractDate(line)\n",
    "            start = False\n",
    "        elif(end):\n",
    "            transitions.append((name, last, extractDate(line)))\n",
    "            end = False\n",
    "    f.close()\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatches(file):\n",
    "    result = []\n",
    "    f = open(file, \"r\")\n",
    "    pattern = re.compile(\"Received [0-9]+ records for topic t[0-9]{1}$\")\n",
    "    for line in f.readlines():\n",
    "        if(pattern.search(line)):\n",
    "            t = line[line.index(\"Received\") + 9:]\n",
    "            t = t[:t.index(\" \")]\n",
    "            result.append(int(t))\n",
    "    \n",
    "    f.close()\n",
    "    return result\n",
    "\n",
    "def removeDouble(df, column, df_name, name):\n",
    "    l = len(df)\n",
    "    df = df.drop_duplicates(subset = column)\n",
    "    if(l != len(df)):\n",
    "        print(\"Deleted %i doubles for %s of %s\" %((l - len(df)),df_name, name))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    \n",
    "    names = []\n",
    "    produced_generated = []\n",
    "    produced_recieved = []\n",
    "    filtered = []\n",
    "    warnings = []\n",
    "    modelchange = []\n",
    "    firstTimestamp = []\n",
    "    producerRun = []\n",
    "    kafkaBatches = []\n",
    "    transitions = {}\n",
    "\n",
    "    producerByRun = []\n",
    "    for r in runs:\n",
    "        pathHelper = path + r + str(os.path.sep) + \"data\" + str(os.path.sep)\n",
    "        \n",
    "        producer = os.listdir(pathHelper)\n",
    "        \n",
    "        for p in producer:\n",
    "            kafkaBatches.append(getBatches(path + r + str(os.path.sep) + \"logs\" + str(os.path.sep) + p + str(os.path.sep) + \"log.log\"))\n",
    "        \n",
    "        producer = [pathHelper + x + str(os.path.sep) for x in producer]\n",
    "        producerByRun.append(producer)\n",
    "        transitions[r] =  transitionsFromFile(path + \"..\" + str(os.path.sep) + \"runs\" + str(os.path.sep) + \"exp\" + str(experiment) + \"-\" + r + \".log\")\n",
    "\n",
    "        for prod in tqdm(producer):\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                time = os.listdir(prod)[0]\n",
    "                topic = list(filter(lambda x : len(x) == 6, os.listdir(prod + time + str(os.path.sep))))[0][:-4]\n",
    "                name = \"run_\" + r + \"_\" + topic\n",
    "                \n",
    "                dataPath = prod  + str(os.path.sep) + time + str(os.path.sep) + topic\n",
    "\n",
    "                produced_generated_file = open(dataPath + \"_produced.csv\")\n",
    "                produced_generated_file_out = open(dataPath + \"_tmp.csv\", \"w\")\n",
    "                \n",
    "                errors = 0\n",
    "                for line in produced_generated_file:\n",
    "                    if(line.startswith(\"org.apache.kafka.common.errors.TimeoutException\")):\n",
    "                        errors += 1\n",
    "                        pass\n",
    "                    elif(line.startswith(\"org.apache.kafka.common\")):\n",
    "                        errors += 1\n",
    "                        print(\"Got %s for %s\" %(line, prod))\n",
    "                    else:\n",
    "                        produced_generated_file_out.write(line)\n",
    "                if(errors > 0):\n",
    "                    print(\"Got %i errors for %s\" %(errors, prod))\n",
    "                \n",
    "                produced_generated_file.close()\n",
    "                produced_generated_file_out.close()\n",
    "                \n",
    "                produced_generated_df = pandas.read_csv(dataPath + \"_tmp.csv\")\n",
    "                \n",
    "                os.remove(dataPath + \"_tmp.csv\") \n",
    "                \n",
    "                produced_generated_df = removeDouble(produced_generated_df,\"Kafka.Offset\", \"produced_generated\", name)\n",
    "\n",
    "                produced_recieved_df = pandas.read_csv(dataPath + \".csv\")\n",
    "                produced_recieved_df = removeDouble(produced_recieved_df,\"Kafka.Offset\", \"produced_recieved\", name)\n",
    "\n",
    "                produced_generated_df = filterLastData(produced_generated_df, produced_recieved_df)\n",
    "\n",
    "                filtered_df = pandas.read_csv(dataPath + \"_filtered.csv\")\n",
    "                filtered_df = removeDouble(filtered_df,\"Kafka.Offset\", \"filtered\", name)\n",
    "                warnings_df = pandas.read_csv(dataPath + \"_warnings.csv\")\n",
    "                warnings_df = removeDouble(warnings_df,\"Kafka.Offset\", \"warnings\", name)\n",
    "                modelchange_df = pandas.read_csv(dataPath + \"_modelchange.csv\")     \n",
    "\n",
    "                #remove first 60100 elements ( around 5 minutes)\n",
    "\n",
    "                modelchange_df = modelchange_df[modelchange_df[\"producedElements\"] > 60101]\n",
    "\n",
    "                producedFilter = produced_generated_df[\"ProducedElements\"] <= 60100 # last point of anomaly\n",
    "\n",
    "                offsets = produced_generated_df[producedFilter][\"Kafka.Offset\"].to_numpy()\n",
    "                produced_generated_df = produced_generated_df[np.logical_not(producedFilter)]\n",
    "\n",
    "                producedReceivedFilter = np.isin(produced_recieved_df[\"Kafka.Offset\"].to_numpy(), offsets)\n",
    "                produced_recieved_df = produced_recieved_df[np.logical_not(producedReceivedFilter)]\n",
    "\n",
    "                filteredFilter = np.isin(filtered_df[\"Data.Offset\"].to_numpy(), offsets)\n",
    "                offsets = filtered_df[\"Kafka.Offset\"][filteredFilter].to_numpy()\n",
    "                filtered_df = filtered_df[np.logical_not(filteredFilter)]\n",
    "\n",
    "                warningFilter = np.isin(warnings_df[\"Record.BeginOffset\"], offsets)\n",
    "                warnings_df = warnings_df[np.logical_not(warningFilter)]\n",
    "\n",
    "                firstTimestampValue = produced_generated_df.iloc()[0][3]\n",
    "    \n",
    "                producerRun.append(r)\n",
    "                produced_generated.append(produced_generated_df)\n",
    "                produced_recieved.append(produced_recieved_df)\n",
    "                filtered.append(filtered_df)\n",
    "                warnings.append(warnings_df)\n",
    "                modelchange.append(modelchange_df)\n",
    "                firstTimestamp.append(firstTimestampValue)\n",
    "\n",
    "                names.append(name)\n",
    "            except:\n",
    "                print(\"error\",prod)\n",
    "                pass\n",
    "\n",
    "    return names, produced_generated, produced_recieved, filtered, warnings, modelchange, firstTimestamp, transitions, producerRun, kafkaBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05e50dbb2784322890c13061949439c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "names, produced_generated, produced_recieved, filtered, warnings, modelchange, firstTimestamp, transitions, producerRun, kafkaBatches = loadData(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(produced_generated[4])-len(produced_generated[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRUN:33\u001b[0m\n",
      "t1\n",
      "{0: {'leader': 'server8', 'replicas': {'server1', 'server9', 'server8'}, 'isrs': {'server1', 'server9', 'server8'}}}\n",
      "\n",
      "t1_filtered\n",
      "{0: {'leader': 'server2', 'replicas': {'server2', 'server3', 'server8'}, 'isrs': {'server2', 'server3', 'server8'}}}\n",
      "\n",
      "t1_warnings\n",
      "{0: {'leader': 'server3', 'replicas': {'server3', 'server7', 'server5'}, 'isrs': {'server3', 'server7', 'server5'}}}\n",
      "\u001b[31m[('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.1 2020-10-25 14:23:34\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "1.00 s after s3 / 20.00 s after start\n",
      "\n",
      "t2\n",
      "{0: {'leader': 'server3', 'replicas': {'server2', 'server3', 'server8'}, 'isrs': {'server2', 'server3', 'server8'}}}\n",
      "\n",
      "t2_filtered\n",
      "{0: {'leader': 'server1', 'replicas': {'server1', 'server3', 'server2'}, 'isrs': {'server1', 'server3', 'server2'}}}\n",
      "\n",
      "t2_warnings\n",
      "{0: {'leader': 'server1', 'replicas': {'server1', 'server3', 'server7'}, 'isrs': {'server1', 'server3', 'server7'}}}\n",
      "\u001b[31m[('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.1 2020-10-25 14:23:34\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "1.00 s after s3 / 20.00 s after start\n",
      "\n",
      "t3\n",
      "{0: {'leader': 'server5', 'replicas': {'server3', 'server7', 'server5'}, 'isrs': {'server3', 'server7', 'server5'}}}\n",
      "\u001b[31m[('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.2 2020-10-25 14:23:38\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "5.00 s after s3 / 24.00 s after start\n",
      "\n",
      "t3_filtered\n",
      "{0: {'leader': 'server3', 'replicas': {'server1', 'server3', 'server9'}, 'isrs': {'server1', 'server3', 'server9'}}}\n",
      "\n",
      "t3_warnings\n",
      "{0: {'leader': 'server2', 'replicas': {'server2', 'server1', 'server9'}, 'isrs': {'server2', 'server1', 'server9'}}}\n",
      "\n",
      "t4\n",
      "{0: {'leader': 'server7', 'replicas': {'server7', 'server9', 'server5'}, 'isrs': {'server7', 'server9', 'server5'}}}\n",
      "\u001b[31m[('change', [0, 'leader'], ('server7', 'server9')), ('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.1 2020-10-25 14:23:34\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "1.00 s after s3 / 20.00 s after start\n",
      "\u001b[31m[('change', [0, 'leader'], ('server9', 'server7'))]\u001b[0m\n",
      "Minute 20.3 2020-10-25 14:27:45\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "252.00 s after s3 / 271.00 s after start\n",
      "\n",
      "t4_filtered\n",
      "{0: {'leader': 'server2', 'replicas': {'server2', 'server7', 'server5'}, 'isrs': {'server2', 'server7', 'server5'}}}\n",
      "\u001b[31m[('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.1 2020-10-25 14:23:34\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "1.00 s after s3 / 20.00 s after start\n",
      "\n",
      "t4_warnings\n",
      "{0: {'leader': 'server7', 'replicas': {'server1', 'server7', 'server8'}, 'isrs': {'server1', 'server7', 'server8'}}}\n",
      "\u001b[31m[('change', [0, 'leader'], ('server7', 'server8')), ('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.1 2020-10-25 14:23:34\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "1.00 s after s3 / 20.00 s after start\n",
      "\u001b[31m[('change', [0, 'leader'], ('server8', 'server7'))]\u001b[0m\n",
      "Minute 20.3 2020-10-25 14:27:45\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "252.00 s after s3 / 271.00 s after start\n",
      "\n",
      "t5\n",
      "{0: {'leader': 'server7', 'replicas': {'server3', 'server7', 'server5'}, 'isrs': {'server3', 'server7', 'server5'}}}\n",
      "\u001b[31m[('change', [0, 'leader'], ('server7', 'server5')), ('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.2 2020-10-25 14:23:38\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "5.00 s after s3 / 24.00 s after start\n",
      "\u001b[31m[('change', [0, 'leader'], ('server5', 'server7'))]\u001b[0m\n",
      "Minute 20.3 2020-10-25 14:27:45\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "252.00 s after s3 / 271.00 s after start\n",
      "\n",
      "t5_filtered\n",
      "{0: {'leader': 'server8', 'replicas': {'server3', 'server7', 'server8'}, 'isrs': {'server3', 'server7', 'server8'}}}\n",
      "\u001b[31m[('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.2 2020-10-25 14:23:42\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "9.00 s after s3 / 28.00 s after start\n",
      "\n",
      "t5_warnings\n",
      "{0: {'leader': 'server3', 'replicas': {'server1', 'server3', 'server8'}, 'isrs': {'server1', 'server3', 'server8'}}}\n",
      "\n",
      "t6\n",
      "{0: {'leader': 'server2', 'replicas': {'server2', 'server5', 'server9'}, 'isrs': {'server2', 'server5', 'server9'}}}\n",
      "\n",
      "t6_filtered\n",
      "{0: {'leader': 'server8', 'replicas': {'server2', 'server1', 'server8'}, 'isrs': {'server2', 'server1', 'server8'}}}\n",
      "\n",
      "t6_warnings\n",
      "{0: {'leader': 'server7', 'replicas': {'server2', 'server7', 'server8'}, 'isrs': {'server2', 'server7', 'server8'}}}\n",
      "\u001b[31m[('change', [0, 'leader'], ('server7', 'server2')), ('remove', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\u001b[31m[('add', [0, 'isrs'], [(0, {'server7'})])]\u001b[0m\n",
      "Minute 16.1 2020-10-25 14:23:34\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "1.00 s after s3 / 20.00 s after start\n",
      "\u001b[31m[('change', [0, 'leader'], ('server2', 'server7'))]\u001b[0m\n",
      "Minute 20.3 2020-10-25 14:27:45\n",
      "('s3', 1603632194000, 1603632213000)\n",
      "252.00 s after s3 / 271.00 s after start\n",
      "\n",
      "t7\n",
      "{0: {'leader': 'server2', 'replicas': {'server2', 'server3', 'server8'}, 'isrs': {'server2', 'server3', 'server8'}}}\n",
      "\n",
      "t7_filtered\n",
      "{0: {'leader': 'server1', 'replicas': {'server1', 'server5', 'server9'}, 'isrs': {'server1', 'server5', 'server9'}}}\n",
      "\n",
      "t7_warnings\n",
      "{0: {'leader': 'server9', 'replicas': {'server2', 'server3', 'server9'}, 'isrs': {'server2', 'server3', 'server9'}}}\n",
      "\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "analyst-0\n",
      "{'state': 'Running', 'server': 'server8'}\n",
      "\n",
      "analyst-1\n",
      "{'state': 'Running', 'server': 'server3'}\n",
      "\n",
      "analyst-2\n",
      "{'state': 'Running', 'server': 'server9'}\n",
      "\n",
      "analyst-3\n",
      "{'state': 'Running', 'server': 'server4'}\n",
      "\n",
      "analyst-4\n",
      "{'state': 'Running', 'server': 'server1'}\n",
      "\n",
      "analyst-5\n",
      "{'state': 'Running', 'server': 'server7'}\n",
      "\u001b[31m[('change', 'state', ('Running', 'Pending')), ('change', 'server', ('server7', 'server5'))]\u001b[0m\n",
      "Minute 10.6 2020-10-25 14:18:03\n",
      "\u001b[31m[('change', 'state', ('Pending', 'Running'))]\u001b[0m\n",
      "Minute 10.7 2020-10-25 14:18:12\n",
      "\n",
      "analyst-6\n",
      "{'state': 'Running', 'server': 'server6'}\n",
      "\n",
      "filter-0\n",
      "{'state': 'Running', 'server': 'server8'}\n",
      "\n",
      "filter-1\n",
      "{'state': 'Running', 'server': 'server3'}\n",
      "\n",
      "filter-2\n",
      "{'state': 'Running', 'server': 'server9'}\n",
      "\n",
      "filter-3\n",
      "{'state': 'Running', 'server': 'server5'}\n",
      "\n",
      "filter-4\n",
      "{'state': 'Running', 'server': 'server2'}\n",
      "\n",
      "filter-5\n",
      "{'state': 'Running', 'server': 'server4'}\n",
      "\n",
      "filter-6\n",
      "{'state': 'Running', 'server': 'server1'}\n",
      "\n",
      "kafka-0\n",
      "{'state': 'Running', 'server': 'server9'}\n",
      "\n",
      "kafka-1\n",
      "{'state': 'Running', 'server': 'server3'}\n",
      "\n",
      "kafka-2\n",
      "{'state': 'Running', 'server': 'server8'}\n",
      "\n",
      "kafka-3\n",
      "{'state': 'Running', 'server': 'server1'}\n",
      "\n",
      "kafka-4\n",
      "{'state': 'Running', 'server': 'server7'}\n",
      "\n",
      "kafka-5\n",
      "{'state': 'Running', 'server': 'server5'}\n",
      "\n",
      "kafka-6\n",
      "{'state': 'Running', 'server': 'server2'}\n",
      "\n",
      "producer-2vxkr\n",
      "{'state': 'Running', 'server': 'server2'}\n",
      "\n",
      "producer-6wblq\n",
      "{'state': 'Running', 'server': 'server1'}\n",
      "\n",
      "producer-989s4\n",
      "{'state': 'Running', 'server': 'server6'}\n",
      "\n",
      "producer-bbsd4\n",
      "{'state': 'Running', 'server': 'server5'}\n",
      "\n",
      "producer-cqptj\n",
      "{'state': 'Running', 'server': 'server4'}\n",
      "\n",
      "producer-ssmpq\n",
      "{'state': 'Running', 'server': 'server3'}\n",
      "\n",
      "producer-tlql6\n",
      "{'state': 'Running', 'server': 'server7'}\n",
      "\n",
      "zoo-0\n",
      "{'state': 'Running', 'server': 'server10'}\n",
      "\n",
      "zoo-1\n",
      "{'state': 'Running', 'server': 'server4'}\n",
      "\n",
      "zoo-2\n",
      "{'state': 'Running', 'server': 'server1'}\n",
      "\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "def addToData(data, topic, current, seconds, absTime):\n",
    "    #empty\n",
    "    if not current:\n",
    "        return data\n",
    "    \n",
    "    if topic in data:\n",
    "        cd = data[topic]\n",
    "        if cd[\"data\"][-1] != current:\n",
    "            cd[\"data\"].append(current)\n",
    "            cd[\"time\"].append(seconds)\n",
    "            cd[\"abstime\"].append(absTime)\n",
    "    else:\n",
    "        data[topic] = {\n",
    "            \"data\" : [current],\n",
    "            \"time\" : [seconds],\n",
    "            \"abstime\" : [absTime]\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def showDiff(data, transition):\n",
    "    keys = list(data.keys())\n",
    "    keys.sort() \n",
    "    for t in keys:\n",
    "        print(t)\n",
    "        print(data[t][\"data\"][0]) \n",
    "        if len(data[t][\"data\"]) > 1:\n",
    "            ld = data[t][\"data\"][0]\n",
    "            for x in range(1, len(data[t][\"data\"])):\n",
    "                cd = data[t][\"data\"][x]  \n",
    "                print(\"\\x1b[31m\" + str(list(dictdiffer.diff(ld,cd))) + \"\\x1b[0m\")\n",
    "                print(\"Minute %.1f %s\" %(data[t][\"time\"][x] / 60, datetime.fromtimestamp(data[t][\"abstime\"][x] / 1000)))\n",
    "                ltr = None\n",
    "                for tr in transition:\n",
    "                    if(tr[1] > data[t][\"abstime\"][x]):\n",
    "                        break\n",
    "                    ltr = tr\n",
    "                if ltr[2] <= data[t][\"abstime\"][x]:\n",
    "                    print(ltr)\n",
    "                    print(\"%.2f s after %s / %.2f s after start\" %((data[t][\"abstime\"][x] - ltr[2]) / 1000,ltr[0], (data[t][\"abstime\"][x] - ltr[1]) / 1000))\n",
    "                ld = cd\n",
    "        print()\n",
    "        \n",
    "def replaceKafkaWithNode(c, pods, second):\n",
    "    result = []\n",
    "    for x in c:\n",
    "        if(x == -1):\n",
    "            result.append(\"No\")\n",
    "        else:\n",
    "            d = pods[\"kafka-\" + str(x)]\n",
    "            index = 0\n",
    "            for y in range(len(d[\"time\"])):\n",
    "                if(d[\"time\"][y] > second):\n",
    "                    break\n",
    "                index = y\n",
    "            result.append(d[\"data\"][index][\"server\"])\n",
    "    return set(result)\n",
    "\n",
    "def extractResponsibilities(lines, transition):\n",
    "    \n",
    "    trial = 0\n",
    "    seconds = 0\n",
    "    topic = None\n",
    "    partitions = None\n",
    "    kafka = True\n",
    "    \n",
    "    data = {}\n",
    "    pods = {}\n",
    "    \n",
    "    current = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        split = list(filter(lambda x : len(x) > 0, line.split(\" \")))\n",
    "        if(line.startswith(\"Trial \")):\n",
    "            kafka = True\n",
    "            trial = int(split[1])\n",
    "            seconds = int(split[3])\n",
    "            absTime = int(split[5]) * 1000\n",
    "            current = {}\n",
    "        elif(line.startswith(\"  topic \\\"\")):\n",
    "            data = addToData(data, topic, current, seconds, absTime)\n",
    "            topic = split[1][1:-1]\n",
    "            partitions = int(split[3])\n",
    "            current = {}\n",
    "        elif(line.startswith(\"    partition \")):\n",
    "            partition = int(split[1][:-1]) \n",
    "            leader =  int(split[3][:-1]) \n",
    "            replicas = set([int(x) for x in split[5][:-1].split(\",\")])\n",
    "            isrs = set([int(x) for x in split[7][:-1].split(\",\")])\n",
    "            current[partition] = {\n",
    "                \"leader\" : leader,\n",
    "                \"replicas\" : replicas,\n",
    "                \"isrs\" : isrs\n",
    "            }\n",
    "        elif(line.startswith(\"pod \\\"debug\")):\n",
    "            data = addToData(data, topic, current, seconds, absTime)\n",
    "        elif(line.startswith(\"NAME\")):\n",
    "            kafka = False\n",
    "        elif(not kafka):\n",
    "            if(line.startswith((\"analyst\", \"filter\", \"kafka\", \"producer\", \"zoo\"))):\n",
    "                pod = split[0]\n",
    "                state = split[1]\n",
    "                server = split[2][:-1]\n",
    "                \n",
    "                current = {\n",
    "                    \"state\" : state,\n",
    "                    \"server\": server\n",
    "                }\n",
    "                \n",
    "                if pod in pods:\n",
    "                    if pods[pod][\"data\"][-1] != current:\n",
    "                        pods[pod][\"data\"].append(current)\n",
    "                        pods[pod][\"time\"].append(seconds)\n",
    "                        pods[pod][\"abstime\"].append(absTime)\n",
    "                else:\n",
    "                    pods[pod] = {\n",
    "                        \"data\" : [current],\n",
    "                        \"time\" : [seconds],\n",
    "                        \"abstime\" : [absTime]\n",
    "                    }\n",
    "                \n",
    "            else:\n",
    "                if(line.startswith(\"debug\")):\n",
    "                    pass\n",
    "                else:\n",
    "                    print(\"Error\", line)\n",
    "\n",
    "    for t in data:\n",
    "        for i in range(len(data[t][\"data\"])):\n",
    "            c = data[t][\"data\"][i]\n",
    "            time = data[t][\"time\"][i]\n",
    "            for partition in c:\n",
    "                \n",
    "                c[partition][\"replicas\"] = replaceKafkaWithNode(c[partition][\"replicas\"], pods, time)\n",
    "                c[partition][\"isrs\"] = replaceKafkaWithNode(c[partition][\"isrs\"], pods, time)\n",
    "                c[partition][\"leader\"] = list(replaceKafkaWithNode([c[partition][\"leader\"]], pods, 0))[0]\n",
    "               \n",
    "    showDiff(data, transition)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"====================================\")\n",
    "        \n",
    "    showDiff(pods, transition)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"====================================\")\n",
    "\n",
    "for run in runs:    \n",
    "    print(\"\\x1b[34mRUN:\" + str(run) + \"\\x1b[0m\")\n",
    "    p = path + run + os.path.sep + \"logs\" + os.path.sep + \"responsibilities.log\"\n",
    "    file = open(p, 'r') \n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    extractResponsibilities(lines, transitions[run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 33 state initial started 2020-10-25 14:07:30 until 2020-10-25 14:07:53\n",
      "run 33 state s2 started 2020-10-25 14:17:53 until 2020-10-25 14:18:13\n",
      "run 33 state s3 started 2020-10-25 14:23:14 until 2020-10-25 14:23:33\n",
      "run 33 state initial started 2020-10-25 14:30:34 until 2020-10-25 14:30:48\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "for k in transitions:\n",
    "    for t in transitions[k]:\n",
    "        print(\"run %s state %s started %s until %s\" %(k,t[0], datetime.fromtimestamp(t[1] / 1000), datetime.fromtimestamp(t[2] / 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformTimestamp(timestamps, firstTimestamp):\n",
    "    return (timestamps - firstTimestamp) / 60000\n",
    "\n",
    "def plotDistributionOverTime(data, timestamps, path, firstTimestamp, transitions = None):\n",
    "    if(transitions is not None):\n",
    "        for t in transitions:\n",
    "            if t[1] > firstTimestamp and t[1] < timestamps[-1]:\n",
    "                plt.axvspan(transformTimestamp(t[1],firstTimestamp), transformTimestamp(t[2],firstTimestamp), facecolor='r', alpha=0.2)    \n",
    "\n",
    "    plt.plot(transformTimestamp(timestamps,firstTimestamp), data)\n",
    "    plt.xlabel(\"time in min\")\n",
    "    plt.ylabel(\"duration in ms\")\n",
    "    plt.savefig(path + \"_over_time.pdf\")\n",
    "    plt.savefig(path + \"_over_time.jpg\", dpi = 300)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def plotDistributionCumulative(data, path = None):    \n",
    "    fig, ax = plt.subplots()\n",
    "    sorted = np.sort(data)\n",
    "    plt.xscale(\"log\")\n",
    "\n",
    "    plt.plot(sorted,np.linspace(0, 1,len(sorted),endpoint=True))\n",
    "    plt.xlabel(\"duration in ms\")\n",
    "    plt.ylabel(\"cumulative frequency\")\n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_major_formatter(StrMethodFormatter('{x:.0f}'))\n",
    "    ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "    if(path is not None):\n",
    "        plt.savefig(path + \"_dist.pdf\")\n",
    "        plt.savefig(path + \"_dist.jpg\", dpi = 300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAvgMedStdMinMaxFromArray(diff, timestamps, path, name, firstTimestamp, transitions):\n",
    "    plotDistributionOverTime(diff, timestamps, path + name, firstTimestamp, transitions)\n",
    "        \n",
    "    plotDistributionCumulative(diff, path + name)\n",
    "    \n",
    "    avg, med, std, minimum, maximum, per90, per95, per99, per99 = (np.average(diff), np.median(diff), np.std(diff), np.min(diff), np.max(diff), np.percentile(diff, 90), np.percentile(diff, 95), np.percentile(diff, 99), np.percentile(diff, 99.9))\n",
    "    datas = [str(x) for x in [name,avg, med, std, minimum, maximum, per90, per95, per99, per99]]\n",
    "    file_object = open(path + 'values.csv', 'a')\n",
    "    file_object.write(\";\".join(datas) + '\\n')\n",
    "    file_object.close()\n",
    "    return diff\n",
    "    #return \"avg = %.2f ms; median %.2f ms; std %.2f ms; min %d ms; max %d ms; 90%% %.2f ms; 95%% %.2f ms; 99%% %.2f ms; 99.9%% %.2f ms\"\\\n",
    "    #    %(**datas)\n",
    "    \n",
    "def extractAvgMedStdMinMaxFromListOfArray(inputList,path, unit = \"ms\"):\n",
    "    diff = np.concatenate(inputList, axis = 0)\n",
    "    \n",
    "    plotDistributionCumulative(diff, path = path)\n",
    "    \n",
    "    datas = (np.average(diff), np.median(diff), np.std(diff), np.min(diff), np.max(diff), np.percentile(diff, 90), np.percentile(diff, 95), np.percentile(diff, 99), np.percentile(diff, 99.9))\n",
    "    text = \"avg = %.2f \" + unit + \"; median %.2f \" + unit + \"; std %.2f \" + unit + \"; min %d \" + unit + \"; max %d \" + unit + \"; 90%% %.2f \" + unit + \"; 95%% %.2f \" + unit + \"; 99%% %.2f \" + unit + \"; 99.9%% %.2f \" + unit\n",
    "    print(text %(datas))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDictory(path):\n",
    "    shutil.rmtree(path,ignore_errors=True)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    file_object = open(path + 'values.csv', 'w')\n",
    "    file_object.write(\";\".join([\"name\", \"avg\", \"med\", \"std\", \"minimum\", \"maximum\", \"per90\", \"per95\", \"per99\", \"per99\"]) + '\\n')\n",
    "    file_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of generated data over time\n",
    "\n",
    "consumer & produducer timestamp of the produced data\n",
    "\n",
    "data isn't produced every 5ms instead there are peaks and lows, just the avg is 5ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a86efb70514c378dba90d436d63dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".jpg\", dpi = 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:28: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".pdf\")\n",
      "<ipython-input-16-b0178d711975>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".jpg\", dpi = 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:28: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".pdf\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".jpg\", dpi = 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".jpg\", dpi = 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".jpg\", dpi = 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b0178d711975>:28: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(p + \".pdf\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_33_t7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plotHist(data, label, firstTimestamp, transitions, width = 250, slabel = False):\n",
    "    timespan = data[-1] - data[0]\n",
    "    \n",
    "    if(transitions is not None):\n",
    "        labeled = False\n",
    "        for t in transitions:\n",
    "            if t[1] > firstTimestamp and t[1] < data[-1]:\n",
    "                plt.axvspan(transformTimestamp(t[1],firstTimestamp) * 60, transformTimestamp(t[2],firstTimestamp) * 60, facecolor='r', alpha=0.2, label = \"MockFog's transition phase\" if not labeled else None)    \n",
    "                labeled = True\n",
    "    \n",
    "    plt.hist((data - firstTimestamp) / 1000, bins = int(timespan / width),zorder=2, label = label)\n",
    "    plt.hlines(width / 5, 0, (data[-1] - firstTimestamp) / 1000, color = \"red\", zorder=1, label = \"expected amount of points\" if slabel else None)\n",
    "    #plt.title(\"Gaussian Histogram\")\n",
    "    plt.xlabel(\"runtime of the experiment in s\")\n",
    "    plt.ylabel(\"data points in an 250 ms intervall\")\n",
    "    plt.legend()\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        producedTime = produced_generated[x].to_numpy()[:,3]\n",
    "        #print(\"Produced data distribution\")\n",
    "        plotHist(producedTime, \"produced data points\", firstTimestamp[x], transitions[producerRun[x]], slabel = True) \n",
    "        #print(\"Recieved produced data distribution\")\n",
    "        plotHist(produced_recieved[x].to_numpy()[:,0], \"recieved data points\", firstTimestamp[x], None)\n",
    "        p = output + \"produced_hist\" + os.path.sep\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "        p += names[x]\n",
    "        plt.savefig(p + \".pdf\")\n",
    "        plt.savefig(p + \".jpg\", dpi = 300)\n",
    "        plt.close()\n",
    "        print(names[x])\n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often aggregates Kafka data for produced_recieved\n",
      "Modus: 3 with 16.92%\n",
      "One element with 2.81%\n",
      "<= 5 elements 52.71%\n",
      "avg = 12.67 r/s; median 5.00 r/s; std 27.60 r/s; min 1 r/s; max 2000 r/s; 90% 47.00 r/s; 95% 50.00 r/s; 99% 87.00 r/s; 99.9% 140.00 r/s\n"
     ]
    }
   ],
   "source": [
    "def kafkaAggCount(dataList, index, label, path):\n",
    "    print(\"How often aggregates Kafka data for\", label)\n",
    "    counts = np.array([item for sublist in dataList for item in sublist])\n",
    "    counts = counts[counts > 0]\n",
    "    a,b = np.unique(counts, return_counts=True)\n",
    "    countsSum = np.sum(b)\n",
    "    #print(a,b)\n",
    "    print(\"Modus: %d with %.2f%%\" %(a[np.where(b == np.max(b))][0], (np.max(b) / countsSum) * 100))\n",
    "    print(\"One element with %.2f%%\" %((b[np.where(a == 1)][0] / countsSum) * 100))\n",
    "    print(\"<= 5 elements %.2f%%\" %((np.sum(b[np.where(a <= 5)]) / countsSum) * 100))\n",
    "    extractAvgMedStdMinMaxFromListOfArray([counts], path, \"r/s\")\n",
    "\n",
    "kafkaAggCount(kafkaBatches, 0, \"produced_recieved\", output + \"produced_records_sub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on the time difference between the arival time of records for all 3 topics\n",
    "\n",
    "peaks caused by cpu time, it is not regulary produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f73ca0ad144424952d83fc3d95c232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "12\n",
      "35\n",
      "35\n",
      "\n",
      "Arival diff of generated data\n",
      "avg = 5.00 ms; median 5.00 ms; std 1.14 ms; min 0 ms; max 109 ms; 90% 6.00 ms; 95% 6.00 ms; 99% 8.00 ms; 99.9% 15.00 ms\n",
      "Percentage of 5ms 67.85\n",
      "Percentage of 0ms 0.90\n",
      "Percentage of <=10ms 99.63\n",
      "Percentage of >100ms 0.00\n",
      "\n",
      "Arival diff of generated data from Kafka\n",
      "avg = 5.01 ms; median 0.00 ms; std 291.40 ms; min 0 ms; max 324295 ms; 90% 0.00 ms; 95% 18.00 ms; 99% 223.00 ms; 99.9% 297.00 ms\n",
      "Arival diff of filtered data from Kafka\n",
      "avg = 5.01 ms; median 0.00 ms; std 287.79 ms; min 0 ms; max 319740 ms; 90% 0.00 ms; 95% 5.00 ms; 99% 225.00 ms; 99.9% 439.00 ms\n",
      "Arival diff of warning data from Kafka\n",
      "avg = 242.78 ms; median 0.00 ms; std 3219.83 ms; min 0 ms; max 347403 ms; 90% 0.00 ms; 95% 23.15 ms; 99% 508.45 ms; 99.9% 24788.97 ms\n"
     ]
    }
   ],
   "source": [
    "def avgMedStdArivalTime(df, column, path, name, firstTimestamp, transitions, filterArray = None):\n",
    "    df = df.to_numpy()[:,column].astype(np.int64)\n",
    "    diff = df[1:,]-df[:-1,]\n",
    "    timestamps = df[1:,]\n",
    "    if filterArray is not None:\n",
    "        diff = diff[filterArray]\n",
    "        timestamps = timestamps[filterArray]\n",
    "    diff = extractAvgMedStdMinMaxFromArray(diff, timestamps, path, name, firstTimestamp, transitions)\n",
    "    return diff\n",
    "    \n",
    "\n",
    "currentPath = output + \"arivalTimesOfData\" + os.path.sep\n",
    "shutil.rmtree(currentPath,ignore_errors=True)\n",
    "pathesWithoutSep = [currentPath + \"produced_generated\", currentPath + \"produced_received\", currentPath + \"filtered\", currentPath + \"warnings\"] \n",
    "pathes = [x + os.path.sep for x in pathesWithoutSep]\n",
    "for x in pathes:\n",
    "    prepareDictory(x)\n",
    "    \n",
    "pro_gen = []\n",
    "pro_rec = []\n",
    "fil = []\n",
    "war = []\n",
    "    \n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        pro_gen.append(avgMedStdArivalTime(produced_generated[x], 3,pathes[0] + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]]))\n",
    "        pro_rec.append(avgMedStdArivalTime(produced_recieved[x], 0, pathes[1] + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]]))\n",
    "        fil.append(avgMedStdArivalTime(filtered[x], 0, pathes[2] + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]]))\n",
    "        offsetsAtModelChanges = modelchange[6].set_index(\"producedElements\").join(produced_generated[6].set_index(\"ProducedElements\"))[\"Kafka.Offset\"].to_numpy()\n",
    "        warningFilter = np.logical_not(np.isin(warnings[x][\"Record.BeginOffset\"].to_numpy(), offsetsAtModelChanges)[1:])\n",
    "        print(np.sum(np.isin(warnings[x][\"Record.BeginOffset\"].to_numpy(), offsetsAtModelChanges)[1:]))\n",
    "        war.append(avgMedStdArivalTime(warnings[x], 0, pathes[3] + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]], warningFilter))\n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        e.print_stack()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "\n",
    "print(\"Arival diff of generated data\")\n",
    "diff = extractAvgMedStdMinMaxFromListOfArray(pro_gen, pathesWithoutSep[0])\n",
    "print(\"Percentage of 5ms %.2f\" %((np.sum(diff == 5)/len(diff) * 100)))\n",
    "print(\"Percentage of 0ms %.2f\" %((np.sum(diff == 0)/len(diff) * 100)))\n",
    "print(\"Percentage of <=10ms %.2f\" %((np.sum(diff <= 10)/len(diff) * 100)))\n",
    "print(\"Percentage of >100ms %.2f\" %((np.sum(diff > 100)/len(diff) * 100)))\n",
    "print()\n",
    "print(\"Arival diff of generated data from Kafka\")\n",
    "extractAvgMedStdMinMaxFromListOfArray(pro_rec, pathesWithoutSep[1])\n",
    "print(\"Arival diff of filtered data from Kafka\")\n",
    "extractAvgMedStdMinMaxFromListOfArray(fil, pathesWithoutSep[2])\n",
    "print(\"Arival diff of warning data from Kafka\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(war, pathesWithoutSep[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 5ms 67.85\n",
      "Percentage of 0ms 0.90\n",
      "Percentage of <=10ms 99.63\n",
      "Percentage of >50ms 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of 5ms %.2f\" %((np.sum(diff == 5)/len(diff) * 100)))\n",
    "print(\"Percentage of 0ms %.2f\" %((np.sum(diff == 0)/len(diff) * 100)))\n",
    "print(\"Percentage of <=10ms %.2f\" %((np.sum(diff <= 10)/len(diff) * 100)))\n",
    "print(\"Percentage of >50ms %.2f\" %((np.sum(diff > 50)/len(diff) * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long does it take until a produced record is acknowledged by kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fada5b2ceda4689a9c6a4dad3e7e6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Produced acknowledged by Kafka\n",
      "avg = 8749.93 ms; median 122.00 ms; std 42268.85 ms; min 1 ms; max 324521 ms; 90% 424.00 ms; 95% 17898.25 ms; 99% 263782.05 ms; 99.9% 318334.51 ms\n"
     ]
    }
   ],
   "source": [
    "def kafkaAck(df, path, name, firstTimestamp, transitions):\n",
    "    ack = df.to_numpy()[:,2].astype(np.int64)\n",
    "    send = df.to_numpy()[:,3].astype(np.int64)\n",
    "    diff = ack - send\n",
    "    diff = extractAvgMedStdMinMaxFromArray(diff, send, path, name, firstTimestamp, transitions)\n",
    "    return diff\n",
    "\n",
    "\n",
    "currentPath = output + \"producedAckByKafka\"\n",
    "prepareDictory(currentPath + os.path.sep)\n",
    "\n",
    "kaf = []\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        kaf.append(kafkaAck(produced_generated[x], currentPath + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]]))\n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "\n",
    "print(\"Produced acknowledged by Kafka\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(kaf, currentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Kafka.Offset', 'Kafka.Timestamp', 'Kafka.acktime.local',\n",
       "       'Producer.Timestamp', 'ProducedElements'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = produced_recieved[0].to_numpy()[:,0]-produced_recieved[0].to_numpy()[:,3]\n",
    "np.where(a == np.max(a))\n",
    "produced_recieved[0].to_numpy()[:,3][64561]\n",
    "a[64561:64561+10]\n",
    "produced_generated[0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check validity of produced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b3128582c34c1985f8a0f38335c55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now [124999 124999] was lower or equal than last [124999 124999] for field ['Record.BeginOffset' 'Record.EndOffset'], diff was [0 0]\n",
      "Not valid warnings run_33_t2\n",
      "\n",
      "Now [125029 125029] was lower or equal than last [125029 125029] for field ['Record.BeginOffset' 'Record.EndOffset'], diff was [0 0]\n",
      "Not valid warnings run_33_t1\n",
      "\n",
      "Now was doing state s2\n",
      "Last was doing state s2\n",
      "Now [1603631880984 1603631880984] was lower than last [1603631881010 1603631881010] for field ['Kafka.Time' 'Data.Timestamp'], diff was [-26. -26.]\n",
      "Now was doing state s2\n",
      "Last was doing state s2\n",
      "Now [1603631880984 1603631880984] was lower than last [1603631881010 1603631881010] for field ['Kafka.Time' 'Data.Timestamp'], diff was [-26. -26.]\n",
      "Now was doing state s2\n",
      "Last was doing state s2\n",
      "Now [1603631880984 1603631880984] was lower than last [1603631881010 1603631881010] for field ['Kafka.Time' 'Data.Timestamp'], diff was [-26. -26.]\n",
      "Not valid produced run_33_t5\n",
      "\n",
      "Now [124998] is not increases by 1 last [124992] for field ['Data.Offset'], diff was [6.]\n",
      "Now [125004] is not increases by 1 last [124998] for field ['Data.Offset'], diff was [6.]\n",
      "Now [125010] is not increases by 1 last [125004] for field ['Data.Offset'], diff was [6.]\n",
      "Not valid filtered run_33_t5\n",
      "\n",
      "Now [1603631722757 1603631722757] was lower than last [1603631722797 1603631722797] for field ['Kafka.Time' 'Data.Timestamp'], diff was [-40. -40.]\n",
      "Not valid produced run_33_t4\n",
      "\n",
      "Now [93300] is not increases by 1 last [93291] for field ['Data.Offset'], diff was [9.]\n",
      "Now was doing state s2\n",
      "Last was doing state s2\n",
      "Now [1603631880054 1603631879832] was lower than last [1603631880069 1603631880027] for field ['Kafka.Time' 'Data.Timestamp'], diff was [ -15. -195.]\n",
      "Now [124707] is not increases by 1 last [124746] for field ['Data.Offset'], diff was [-39.]\n",
      "Now was doing state s2\n",
      "Last was doing state s2\n",
      "Now [1603631880054 1603631879832] was lower than last [1603631880069 1603631880027] for field ['Kafka.Time' 'Data.Timestamp'], diff was [ -15. -195.]\n",
      "Now [124707] is not increases by 1 last [124746] for field ['Data.Offset'], diff was [-39.]\n",
      "Now was doing state s2\n",
      "Last was doing state s2\n",
      "Now [1603631880054 1603631879832] was lower than last [1603631880069 1603631880027] for field ['Kafka.Time' 'Data.Timestamp'], diff was [ -15. -195.]\n",
      "Now [124707] is not increases by 1 last [124746] for field ['Data.Offset'], diff was [-39.]\n",
      "Not valid filtered run_33_t4\n",
      "\n",
      "Now [1603632363925 1603632363925] was lower than last [1603632363955 1603632363955] for field ['Kafka.Time' 'Data.Timestamp'], diff was [-30. -30.]\n",
      "Not valid produced run_33_t3\n",
      "\n",
      "Now [221585] is not increases by 1 last [221578] for field ['Data.Offset'], diff was [7.]\n",
      "Not valid filtered run_33_t3\n",
      "\n",
      "Now [60106] is not increases by 1 last [56056] for field ['Kafka.Offset'], diff was [4050.]\n",
      "Not valid produced run_33_t7\n",
      "\n",
      "Now [60101 60106] is not increases by 1 last [56055 56056] for field ['Kafka.Offset' 'Data.Offset'], diff was [4046. 4050.]\n",
      "Not valid filtered run_33_t7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def genError(text, last, now, result, columns, filterValues, transition):\n",
    "    errorIndex = np.where(np.logical_not(result))\n",
    "    formatter = {'float_kind':lambda x: \"%i\" % x}\n",
    "    a = now[filterValues][errorIndex]\n",
    "    b = last[filterValues][errorIndex]\n",
    "    \n",
    "    for t in transition:\n",
    "        if np.all((a >= t[1]) & (a <= t[2])):\n",
    "            print(\"Now was doing state %s\" %t[0])\n",
    "        if np.all((b >= t[1]) & (b <= t[2])):\n",
    "            print(\"Last was doing state %s\" %t[0])\n",
    "        \n",
    "    a = np.array2string(a, precision = 0, formatter=formatter)\n",
    "    b = np.array2string(b, precision = 0, formatter=formatter)\n",
    "    \n",
    "    diff = now[filterValues][errorIndex] - last[filterValues][errorIndex]\n",
    "    text = text %(a ,b , str(columns[filterValues][errorIndex]), diff)\n",
    "    return text\n",
    "\n",
    "def validate(df, heigherFields, heigherOrEqualFields, offset, transition):\n",
    "    \n",
    "    columns = np.array(df.columns)\n",
    "    df = df.to_numpy()#[:,0:4]\n",
    "    \n",
    "    lastRow = df[0]\n",
    "    error = False\n",
    "    \n",
    "    for x in df[1:]:\n",
    "            \n",
    "        if(np.sum(x[heigherFields] > lastRow[heigherFields]) != len(heigherFields)):\n",
    "            print(genError(\"Now %s was lower or equal than last %s for field %s, diff was %s\", lastRow, x, x[heigherFields] > lastRow[heigherFields], columns, heigherFields, transition))\n",
    "            error  = True           \n",
    "           \n",
    "        if(np.sum(x[heigherOrEqualFields] >= lastRow[heigherOrEqualFields]) != len(heigherOrEqualFields)):\n",
    "            print(genError(\"Now %s was lower than last %s for field %s, diff was %s\", lastRow, x, x[heigherOrEqualFields] >= lastRow[heigherOrEqualFields], columns, heigherOrEqualFields, transition))\n",
    "            error  = True  \n",
    "            \n",
    "        if(not np.sum(lastRow[offset] + 1 == x[offset]) == len(offset)):\n",
    "            errorFilter = np.logical_not(lastRow[offset] + 1 == x[offset])\n",
    "            print(genError(\"Now %s is not increases by 1 last %s for field %s, diff was %s\", lastRow, x, lastRow[offset] + 1 == x[offset], columns, offset, transition))\n",
    "            error = True\n",
    "            \n",
    "        lastRow = x\n",
    "    \n",
    "    return not error\n",
    "        \n",
    "    #f.value = 100\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    t = transitions[producerRun[x]]\n",
    "    if not validate(produced_recieved[x],[],[0,1,3],[2], t):\n",
    "        print(\"Not valid produced\", names[x])\n",
    "        print()\n",
    "    if not validate(filtered[x],[],[0,1,4],[2,3], t):\n",
    "        print(\"Not valid filtered\", names[x])\n",
    "        print()\n",
    "    if not validate(warnings[x],[4,5],[0,1,6,7],[2], t):\n",
    "        print(\"Not valid warnings\", names[x])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time between the last step and the next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extractDiffBetweenTwoTables(a,b,keyA,keyB, sortBy, valueA, valueB, dropDouble = False, sort_by = None):\n",
    "    joined = a.set_index(keyA).add_prefix('a_').join(b.set_index(keyB).add_prefix('b_'))\n",
    "    if dropDouble and sort_by is not None:\n",
    "        joined = joined.sort_values(sort_by, ascending = True).reset_index()\n",
    "        if \"index\" in joined.columns:\n",
    "            joined = joined.rename(columns={\"index\": keyA})\n",
    "        l = len(joined)\n",
    "        joined = joined.drop_duplicates(subset = keyA).set_index(keyA)\n",
    "        if l > len(joined):\n",
    "            print(\"Deleted: %i rows\" %(l - len(joined)))\n",
    "    diff = joined['a_' + valueA].to_numpy() - joined['b_' + valueB].to_numpy()\n",
    "    return diff, joined['b_' + valueB].to_numpy(), joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print two together\n",
    "def printTwoTogether(combineIds, namesForIds, store = True):\n",
    "\n",
    "    indexes = [names.index(x) for x in combineIds]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    \n",
    "    colors = [\"b\",\"g\",\"r\",\"c\",\"y\",\"b\",\"m\"]\n",
    "    \n",
    "    for x in range(len(indexes)):\n",
    "        diffPPR, timestamps, _ = extractDiffBetweenTwoTables(produced_recieved[indexes[x]], produced_generated[indexes[x]], 'Kafka.Offset', 'Kafka.Offset', \"Kafka.Offset\", 'Consumer.Time', 'Producer.Timestamp')\n",
    "        #extractAvgMedStdMinMaxFromArray(diffPPR, None, None, None, None)\n",
    "        sorted = np.sort(diffPPR)\n",
    "        plt.plot(sorted,np.linspace(0, 1,len(sorted),endpoint=True), label = namesForIds[x], c = colors[int(names[x][-1:]) - 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel(\"duration in ms\")\n",
    "    plt.ylabel(\"cumulative frequency\")\n",
    "    plt.legend()\n",
    "\n",
    "    ax.xaxis.set_major_formatter(StrMethodFormatter('{x:.0f}'))\n",
    "    ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "    cOutput = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedProdRec_\" + \"-\".join(combineIds)\n",
    "    if store:\n",
    "        plt.savefig(cOutput + \".jpg\", dpi = 300)\n",
    "        plt.savefig(cOutput + \".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "try:\n",
    "    #printTwoTogether(names, names, False)\n",
    "    printTwoTogether([\"run_2_t4\", \"run_0_t4\"], [\"good placement (t4)\", \"poor placement (t4)\"])\n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values are possible, a node can receive the warnings before the filtered, it depends on the position in cluster an other aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currentPathFW = output + \"receivedByFollowingTopic\" + os.path.sep + \"FilteredRecWarning\"\n",
    "prepareDictory(currentPathFW + os.path.sep)\n",
    "\n",
    "currentPathPW = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedRecWarning\"\n",
    "prepareDictory(currentPathPW + os.path.sep)\n",
    "\n",
    "currentPathPGW = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedGenWarning\"\n",
    "prepareDictory(currentPathPGW + os.path.sep)\n",
    "\n",
    "fw = []\n",
    "pw = []\n",
    "pgw = []\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        \n",
    "        #In warnings Record.BeginOffset and Record.EndOffset are the related filtered offsets! There is no guarantee that these values are the same!!\n",
    "        warnings_filtered_joined = warnings[x].set_index(\"Record.BeginOffset\").join(filtered[x].add_prefix('fb_').set_index(\"fb_Kafka.Offset\")).set_index(\"Record.EndOffset\").join(filtered[x].add_prefix('fe_').set_index(\"fe_Kafka.Offset\"))\n",
    "\n",
    "        diff = (warnings_filtered_joined[\"Consumer.Time\"] - warnings_filtered_joined[\"fb_Consumer.Time\"]).to_numpy()\n",
    "        diff, timestamps = removeNaN(diff, warnings_filtered_joined[\"fb_Consumer.Time\"].to_numpy())\n",
    "        diff = extractAvgMedStdMinMaxFromArray(diff, timestamps, currentPathFW + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        fw.append(diff)\n",
    "\n",
    "        diff, timestamps, prod_warning_join = extractDiffBetweenTwoTables(warnings_filtered_joined, produced_recieved[x].add_prefix('pr_'), 'fb_Data.Offset', 'pr_Kafka.Offset', \"pr_Consumer.Time\", 'Consumer.Time', 'pr_Consumer.Time')\n",
    "        diff, timestamps = removeNaN(diff, timestamps)\n",
    "        diff = extractAvgMedStdMinMaxFromArray(diff, timestamps, currentPathPW + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        pw.append(diff)\n",
    "        \n",
    "        prodGenProdRec = produced_generated[x].set_index(\"Kafka.Offset\").join(produced_recieved[x].set_index(\"Kafka.Offset\")).reset_index()\n",
    "        diff, timestamps, prod_warning_join = extractDiffBetweenTwoTables(warnings_filtered_joined, prodGenProdRec.add_prefix('pr_'), 'fb_Data.Offset', 'pr_Kafka.Offset', \"pr_Consumer.Time\", 'Consumer.Time', 'pr_Producer.Timestamp')\n",
    "        diff, timestamps = removeNaN(diff, timestamps)\n",
    "        diff = extractAvgMedStdMinMaxFromArray(diff, timestamps, currentPathPGW + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        pgw.append(diff)\n",
    "        \n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "\n",
    "print(\"Filtered -> Warning\")\n",
    "extractAvgMedStdMinMaxFromListOfArray(fw, currentPathFW)\n",
    "print(\"ProducedRec -> Warning\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(pw, currentPathPW)\n",
    "print(\"ProducedGen -> Warning\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(pgw, currentPathPGW)\n",
    "diff = np.concatenate(pgw, axis = 0)\n",
    "print(\"Amount of anomalies:\", len(diff))\n",
    "print(\"More than 3000 ms: %.2f%%\"%((np.sum(diff > 3000) / len(diff)) * 100))\n",
    "print(\"More than 5000 ms: %.2f%%\"%((np.sum(diff > 5000) / len(diff)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currentPathPF = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedGenFiltered\"\n",
    "prepareDictory(currentPathPF + os.path.sep)\n",
    "currentPathPPR = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedGenProdRec\"\n",
    "prepareDictory(currentPathPPR + os.path.sep)\n",
    "\n",
    "diffListPF = []\n",
    "diffListPPR = []\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        \n",
    "        #remove elements received more than once, identify by the measurement\n",
    "        #prod_rec_clean = produced_recieved[x].drop_duplicates(subset = \"Data.Measurement\")\n",
    "        \n",
    "        #join measurement\n",
    "        produced_generated_with_measurement = produced_generated[x].set_index(\"Kafka.Offset\").join(produced_recieved[x].set_index(\"Kafka.Offset\")[\"Data.Measurement\"])\n",
    "        \n",
    "        PPR_join = produced_generated_with_measurement\\\n",
    "        .set_index(\"Data.Measurement\")\\\n",
    "        .join(produced_recieved[x]\\\n",
    "              .set_index(\"Data.Measurement\")\\\n",
    "            )\\\n",
    "        .sort_values(\"Kafka.Offset\", ascending = True)\\\n",
    "        .reset_index()\\\n",
    "        .drop_duplicates(subset = [\"Producer.Timestamp\",\"Data.Measurement\"])\n",
    "        \n",
    "        #diffPPR, timestamps, PPR_join = extractDiffBetweenTwoTables( prod_rec_clean, produced_generated_with_measurement, \"Data.Measurement\", \"Data.Measurement\", \"b_ProducedElements\", 'Consumer.Time', 'Producer.Timestamp', True, \"a_Consumer.Time\")\n",
    "        diffPPR = (PPR_join[\"Consumer.Time\"] - PPR_join[\"Data.Timestamp\"]).to_numpy()\n",
    "        timestamps = PPR_join[\"Data.Timestamp\"].to_numpy()\n",
    "        diffPPR = extractAvgMedStdMinMaxFromArray(diffPPR, timestamps, currentPathPPR + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "      \n",
    "        s = np.sum(np.isnan(diffPPR))\n",
    "        if(s > 0):\n",
    "            print(\"Did not process %i offsets for produced %s\" %(s, names[x]))\n",
    "            diffPPR = diffPPR[np.logical_not(np.isnan(diffPPR))]\n",
    "            \n",
    "        diffListPPR.append(diffPPR)\n",
    "        \n",
    "        PF_join = PPR_join.set_index(\"Kafka.Offset\").add_prefix(\"a_\")\\\n",
    "        .join(filtered[x].set_index(\"Data.Offset\"))\\\n",
    "        .sort_values(\"Consumer.Time\", ascending = True)\\\n",
    "        .drop_duplicates(subset = \"a_ProducedElements\")\n",
    "        \n",
    "        #diffPF, timestamps, _ = extractDiffBetweenTwoTables(filtered[x], PPR_join.reset_index(), 'Data.Offset', 'a_Kafka.Offset', \"Consumer.Time\", 'Consumer.Time', 'b_Producer.Timestamp', True, \"a_Consumer.Time\")\n",
    "        #print(names[x], np.max(diffPF), timestamps[np.where(np.max(diffPF)==diffPF)])\n",
    "        \n",
    "        \n",
    "        diffPF = (PF_join[\"Consumer.Time\"] - PF_join[\"a_Data.Timestamp\"]).to_numpy()\n",
    "        timestamps = PF_join[\"a_Data.Timestamp\"].to_numpy()\n",
    "        diffPF = extractAvgMedStdMinMaxFromArray(diffPF, timestamps, currentPathPF + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        \n",
    "        s = np.sum(np.isnan(diffPF))\n",
    "        if(s > 0):\n",
    "            print(\"Did not process %i offsets for filtered %s\" %(s, names[x]))\n",
    "            diffPF = diffPF[np.logical_not(np.isnan(diffPF))]\n",
    "            \n",
    "        \n",
    "        diffListPF.append(diffPF)\n",
    "        \n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "\n",
    "        \n",
    "print(\"Produced -> Produced Received\")\n",
    "extractAvgMedStdMinMaxFromListOfArray(diffListPPR, currentPathPPR)\n",
    "diffPPR = np.concatenate(diffListPPR, axis = 0)\n",
    "print(\"More than 100 ms: %.2f%%\"%((np.sum(diffPPR > 100) / len(diffPPR)) * 100))\n",
    "print(\"More lower than 8 ms: %.2f%%\"%((np.sum(diffPPR < 8) / len(diffPPR)) * 100))\n",
    "\n",
    "print(\"Produced -> Filtered\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(diffListPF, currentPathPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPathFW = output + \"receivedByFollowingTopic\" + os.path.sep + \"FilteredRecWarning\"\n",
    "prepareDictory(currentPathFW + os.path.sep)\n",
    "\n",
    "currentPathPW = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedRecWarning\"\n",
    "prepareDictory(currentPathPW + os.path.sep)\n",
    "\n",
    "currentPathPGW = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedGenWarning\"\n",
    "prepareDictory(currentPathPGW + os.path.sep)\n",
    "\n",
    "currentPathPGF = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedGenFiltered\"\n",
    "prepareDictory(currentPathPGF + os.path.sep)\n",
    "\n",
    "currentPathPPR = output + \"receivedByFollowingTopic\" + os.path.sep + \"ProducedGenProdRec\"\n",
    "prepareDictory(currentPathPPR + os.path.sep)\n",
    "\n",
    "currentPathAll = output + \"receivedByFollowingTopic\" + os.path.sep + \"allTogether\" + os.path.sep\n",
    "os.makedirs(currentPathAll, exist_ok=True)\n",
    "\n",
    "fw = []\n",
    "pw = []\n",
    "pgw = []\n",
    "pgf = []\n",
    "ppr = []\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        \n",
    "        produced_generated_with_measurement = produced_generated[x].set_index(\"Kafka.Offset\").join(produced_recieved[x].set_index(\"Kafka.Offset\")[\"Data.Measurement\"]).reset_index()\n",
    "        fullJoin = produced_generated_with_measurement.set_index(\"Data.Measurement\").add_prefix(\"pg_\")\\\n",
    "            .join(produced_recieved[x].set_index(\"Data.Measurement\").add_prefix(\"pr_\"))\\\n",
    "            .reset_index()\\\n",
    "            .set_index(\"pr_Kafka.Offset\")\\\n",
    "            .join(filtered[x].set_index(\"Data.Offset\").add_prefix(\"fi_\"))\\\n",
    "            .set_index(\"fi_Kafka.Offset\")\\\n",
    "            .join(warnings[x].set_index(\"Record.BeginOffset\").add_prefix(\"wa_\"))\\\n",
    "        \n",
    "        #filter wrong joins\n",
    "        fjFilter = np.abs((fullJoin[\"pr_Data.Timestamp\"] - fullJoin[\"pg_Producer.Timestamp\"]).to_numpy()) > 100\n",
    "        fullJoin = fullJoin[np.logical_not(fjFilter)]\n",
    "       \n",
    "        #Prod Gen -> Warning\n",
    "        allJoin = fullJoin\\\n",
    "        .sort_values([\"pg_ProducedElements\", \"wa_Consumer.Time\"], ascending = True)\\\n",
    "        .drop_duplicates(subset = [\"pg_ProducedElements\"])\n",
    "        pgwjoin = allJoin[allJoin[\"Data.Measurement\"]>2.0]\n",
    "        diffPGW = (pgwjoin[\"wa_Consumer.Time\"] - pgwjoin[\"pg_Producer.Timestamp\"]).to_numpy()\n",
    "        timestamps = pgwjoin[\"pg_Producer.Timestamp\"].to_numpy()\n",
    "        diffPGW = extractAvgMedStdMinMaxFromArray(diffPGW, timestamps, currentPathPGW + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        pgw.append(diffPGW)\n",
    "        \n",
    "        #Prod Rec -> Warning\n",
    "        diffPW = (pgwjoin[\"wa_Consumer.Time\"] - pgwjoin[\"pr_Consumer.Time\"]).to_numpy()\n",
    "        timestamps = pgwjoin[\"pr_Consumer.Time\"].to_numpy()\n",
    "        diffPW = extractAvgMedStdMinMaxFromArray(diffPW, timestamps, currentPathPW + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        pw.append(diffPW)\n",
    "        \n",
    "        #Filter -> Warning\n",
    "        diffFW = (pgwjoin[\"wa_Consumer.Time\"] - pgwjoin[\"fi_Consumer.Time\"]).to_numpy()\n",
    "        timestamps = pgwjoin[\"fi_Consumer.Time\"].to_numpy()\n",
    "        diffFW = extractAvgMedStdMinMaxFromArray(diffFW, timestamps, currentPathFW + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        fw.append(diffFW)\n",
    "        \n",
    "        #Prod Gen -> Filter\n",
    "        diffPGF = (allJoin[\"fi_Consumer.Time\"] - allJoin[\"pg_Producer.Timestamp\"]).to_numpy()\n",
    "        timestamps = allJoin[\"pg_Producer.Timestamp\"].to_numpy()\n",
    "        diffPGF = extractAvgMedStdMinMaxFromArray(diffPGF, timestamps, currentPathPGF + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        pgf.append(diffPGF)\n",
    "        \n",
    "        #Prod Gen -> Prod Rec\n",
    "        diffPPR = (allJoin[\"pr_Consumer.Time\"] - allJoin[\"pg_Producer.Timestamp\"]).to_numpy()\n",
    "        timestamps = allJoin[\"pg_Producer.Timestamp\"].to_numpy()\n",
    "        diffPPR = extractAvgMedStdMinMaxFromArray(diffPPR, timestamps, currentPathPPR + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        ppr.append(diffPPR)\n",
    "        \n",
    "        diffFW = (allJoin[\"wa_Consumer.Time\"] - allJoin[\"pg_Producer.Timestamp\"]).to_numpy()\n",
    "        \n",
    "        for t in transitions[producerRun[x]]:\n",
    "            if t[1] > firstTimestamp[x] and t[1] < timestamps[-1]:\n",
    "                plt.axvspan(t[1] - firstTimestamp[x], t[2] - firstTimestamp[x], facecolor='r', alpha=0.2) \n",
    "        \n",
    "        timestamps = timestamps - firstTimestamp[x]\n",
    "        plt.plot(timestamps, diffPPR,label = \"produced\")\n",
    "        plt.plot(timestamps,diffPGF,label = \"filtered\")\n",
    "        plt.plot(timestamps,diffFW,label = \"warning\")\n",
    "        plt.legend()\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.savefig(currentPathAll + names[x] + \".jpg\")\n",
    "        plt.savefig(currentPathAll + names[x] + \".pdf\")\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "\n",
    "print(\"Produced -> Produced Received\")\n",
    "extractAvgMedStdMinMaxFromListOfArray(ppr, currentPathPPR)\n",
    "diffPPR = np.concatenate(ppr, axis = 0)\n",
    "print(\"More than 100 ms: %.2f%%\"%((np.sum(diffPPR > 100) / len(diffPPR)) * 100))\n",
    "print(\"More lower than 8 ms: %.2f%%\"%((np.sum(diffPPR < 8) / len(diffPPR)) * 100))\n",
    "\n",
    "print(\"Produced Gen -> Filtered\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(pgf, currentPathPGF)        \n",
    "        \n",
    "print(\"Filtered -> Warning\")\n",
    "extractAvgMedStdMinMaxFromListOfArray(fw, currentPathFW)\n",
    "print(\"ProducedRec -> Warning\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(pw, currentPathPW)\n",
    "print(\"ProducedGen -> Warning\")\n",
    "_ = extractAvgMedStdMinMaxFromListOfArray(pgw, currentPathPGW)\n",
    "diff = np.concatenate(pgw, axis = 0)\n",
    "print(\"Amount of anomalies:\", len(diff))\n",
    "print(\"More than 3000 ms: %.2f%%\"%((np.sum(diff > 3000) / len(diff)) * 100))\n",
    "print(\"More than 5000 ms: %.2f%%\"%((np.sum(diff > 5000) / len(diff)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loss produced to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037e506b73e2401b87ac4de942569af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calcDataLoss(df1, df1OffsetColumn, df2, df2OffsetColumn):\n",
    "    df1Offsets = df1.to_numpy()[:,df1OffsetColumn].astype(np.int64)\n",
    "    df2Offsets = df2.to_numpy()[:,df2OffsetColumn].astype(np.int64)\n",
    "    \n",
    "    errors = {}\n",
    "        \n",
    "    count = {}\n",
    "    \n",
    "    for x in df1Offsets:\n",
    "        \n",
    "        count[x] = 0\n",
    "\n",
    "        \n",
    "        #count = np.sum(df2Offsets == x)\n",
    "        \n",
    "        #if(count != 1):\n",
    "        #    errors[count] = errors.get(count, 0) + 1\n",
    "        \n",
    "    for x in df1Offsets:\n",
    "        count[x] = count[x] + 1\n",
    "        \n",
    "    for k, v in count.items():\n",
    "        if v != 1:\n",
    "            errors[count] = errors.get(count, 0) + 1\n",
    "    \n",
    "    out = \"Received records \"\n",
    "    losses = False\n",
    "    for k, v in errors.items():\n",
    "        out += \"%ix for %i times; \" %(k,v)\n",
    "        losses = True\n",
    "    \n",
    "    if(losses):\n",
    "        print(out)\n",
    "    #else:\n",
    "    #    print(\"The expected data was received exactly once.\")\n",
    "    return not losses\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        \n",
    "        if not calcDataLoss(produced_generated[x], 0, filtered[x], 3) :\n",
    "            print(\"Error\", names[x])\n",
    "        \n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency produced - warning\n",
    "\n",
    "As the filter uses a median filter with size 5 it needs 3 outliers to detect a change. otherwise it is just skipped. ==> there must be a latency of at least (3*5ms) = 15ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all anomalies were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00036308554e4a02ad7d6a67fd25c47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3535 warnings\n",
      "Found 3535 warnings\n",
      "Found 3535 warnings\n",
      "Found 3535 warnings\n",
      "Found 3535 warnings\n",
      "Found 3535 warnings\n",
      "Found 3535 warnings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        \n",
    "        filterOver10 = filtered[x][filtered[x][\"Data.Measurement\"] > 1.0]\n",
    "        filterOver10Joined = filterOver10.add_prefix('filtered_').set_index(\"filtered_Kafka.Offset\").join(warnings[x].add_prefix('warnings_').set_index(\"warnings_Record.BeginOffset\"))\n",
    "        noBelongingWarning = filterOver10Joined[filterOver10Joined[\"warnings_Consumer.Time\"].isna()]\n",
    "        print(\"Found %d warnings\" %(len(filterOver10)))\n",
    "        if(len(noBelongingWarning) > 0):\n",
    "            print(\"%s: For %d / %d anomalies there were no belonging warnings\" %(names[x], len(noBelongingWarning),len(filterOver10)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979b6562747640c9a4039d970313dc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to detect an anomaly (initial value)\n",
      "avg = 10025.87 ms; median 389.00 ms; std 44846.54 ms; min 93 ms; max 323320 ms; 90% 1038.60 ms; 95% 28760.80 ms; 99% 263969.16 ms; 99.9% 317372.26 ms\n",
      "Amount of anomalies: 245\n"
     ]
    }
   ],
   "source": [
    "currentPath = output + \"anomalyDetectionTime\"\n",
    "prepareDictory(currentPath + os.path.sep)\n",
    "\n",
    "diffList = []\n",
    "\n",
    "for x in nrange(len(names)):\n",
    "    try:\n",
    "        \n",
    "        #received and produced\n",
    "        pr_rc = produced_generated[x].add_prefix('produced_generated_').set_index(\"produced_generated_Kafka.Offset\").join(produced_recieved[x].add_prefix('produced_recieved_').set_index(\"produced_recieved_Kafka.Offset\")).reset_index()\n",
    "    \n",
    "        #print(np.unique(np.unique(produced_recieved[x][\"Kafka.Offset\"].to_numpy(), return_counts=True)[1], return_counts=True))#[\"Kafka.Offset\"])\n",
    "        \n",
    "        if \"index\" in pr_rc.columns:\n",
    "            pr_rc = pr_rc.rename(columns={\"index\": \"produced_generated_Kafka.Offset\"})\n",
    "\n",
    "        #received and produced and filtered\n",
    "        pr_rc_fi = pr_rc.set_index(\"produced_generated_Kafka.Offset\").join(filtered[x].add_prefix('filtered_').set_index(\"filtered_Data.Offset\"))#.reset_index()\n",
    "        \n",
    "        if \"index\" in pr_rc_fi.columns:\n",
    "            pr_rc_fi = pr_rc_fi.rename(columns={\"index\": \"produced_generated_Kafka.Offset\"})\n",
    "\n",
    "        #changes and received and produced and filtered\n",
    "        mc_pg_pr_fi = modelchange[x].add_prefix('modelchange_').set_index(\"modelchange_producedElements\").join(pr_rc_fi.set_index(\"produced_generated_ProducedElements\")).reset_index().rename(columns={'modelchange_producedElements': 'producedElements'})\n",
    "\n",
    "        if \"index\" in mc_pg_pr_fi.columns:\n",
    "            mc_pg_pr_fi = mc_pg_pr_fi.rename(columns={\"index\": \"producedElements\"})\n",
    "        \n",
    "        #merge all with warnings\n",
    "        fullJoin = mc_pg_pr_fi.set_index(\"filtered_Kafka.Offset\").join(warnings[x].add_prefix('warning_').set_index(\"warning_Record.BeginOffset\")).reset_index()\n",
    "        \n",
    "        if \"index\" in fullJoin.columns:\n",
    "            fullJoin = fullJoin.rename(columns={\"index\": \"filtered_Kafka.Offset\"})\n",
    "\n",
    "        fullJoinOver10 = fullJoin[fullJoin[\"modelchange_value\"] > 1]\n",
    "        \n",
    "        fullJoinOver10 = fullJoinOver10.drop_duplicates(subset = \"filtered_Kafka.Offset\")\n",
    "        \n",
    "        diff = (fullJoinOver10['warning_Consumer.Time'] - fullJoinOver10['produced_generated_Producer.Timestamp']).to_numpy()\n",
    "\n",
    "        timestamps = fullJoinOver10['produced_generated_Producer.Timestamp'].to_numpy()\n",
    "\n",
    "        if (np.isnan(diff[-1])):\n",
    "            diff = diff[:-1]\n",
    "            timestamps = timestamps[:-1]\n",
    "\n",
    "        diff = extractAvgMedStdMinMaxFromArray(diff, timestamps, currentPath + os.path.sep, names[x], firstTimestamp[x], transitions[producerRun[x]])\n",
    "        diffList.append(diff)\n",
    "        \n",
    "    except Exception as e:\n",
    "        plt.close()\n",
    "        print(e)\n",
    "        print(\"Error\", names[x], x)\n",
    "\n",
    "print(\"Time to detect an anomaly (initial value)\")\n",
    "diff = extractAvgMedStdMinMaxFromListOfArray(diffList, currentPath)\n",
    "print(\"Amount of anomalies:\", len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractUsage(lines, server, run, transition):\n",
    "    data = {}\n",
    "\n",
    "    date = None\n",
    "    sumMem = 0\n",
    "    sumCPU = 0\n",
    "    for l in lines:\n",
    "        if l.startswith(\"Time \"):\n",
    "            if sumMem > 0:\n",
    "                pid_data = data.get(-1, {\n",
    "                    \"mem\": [],\n",
    "                    \"cpu\": [],\n",
    "                    \"pid\" : -1,\n",
    "                    \"running\" : [],\n",
    "                    \"name\" : \"all\",\n",
    "                    \"container\" : \"all\"\n",
    "                })\n",
    "                pid_data[\"mem\"].append(sumMem)\n",
    "                pid_data[\"cpu\"].append(sumCPU)\n",
    "                pid_data[\"running\"].append(date)\n",
    "                \n",
    "                data[-1] = pid_data\n",
    "            sumMem = 0\n",
    "            sumCPU = 0\n",
    "            date = int(l[5:-1])\n",
    "                \n",
    "        else:\n",
    "            e = l.split(' ');\n",
    "            e = list(filter(lambda x: x != \"\", e))\n",
    "            cpu = float(e[0])\n",
    "            mem = float(e[1])\n",
    "            sumCPU += cpu\n",
    "            sumMem += mem\n",
    "            pid = int(e[2])\n",
    "            time = e[3].split(\":\")\n",
    "            secondsRunning = int(time[0]) * 3600 + int(time[1]) * 60 + int(time[2])\n",
    "            name = e[4]\n",
    "            container = e[5]\n",
    "            up = e[6]\n",
    "            \n",
    "            if(name.startswith(\"etcd\")):\n",
    "                pass\n",
    "            else:\n",
    "                split = name.split(\"_\")\n",
    "                \n",
    "                if split[1] == \"POD\":\n",
    "                    continue\n",
    "                name = split[1] + \"-\" +  split[2]\n",
    "            \n",
    "            pid_data = data.get(pid, {\n",
    "                \"mem\": [],\n",
    "                \"cpu\": [],\n",
    "                \"pid\" : pid,\n",
    "                \"running\" : [],\n",
    "                \"name\" : name,\n",
    "                \"container\" : container\n",
    "            })\n",
    "\n",
    "            pid_data[\"mem\"].append(mem)\n",
    "            pid_data[\"cpu\"].append(cpu)\n",
    "            pid_data[\"running\"].append(date)\n",
    "\n",
    "            data[pid] = pid_data\n",
    "\n",
    "    zero_cpu = []\n",
    "    for k in data:\n",
    "        summ = sum(data[k][\"cpu\"])\n",
    "        if summ == 0.0 or data[k][\"name\"].startswith(\"debug\"):\n",
    "            zero_cpu.append(k)\n",
    "\n",
    "    #for x in zero_cpu:\n",
    "    #    del data[x]\n",
    "\n",
    "    outpath = output + \"usage\" + os.path.sep\n",
    "    \n",
    "    outpathMem = outpath + \"mem\" + os.path.sep\n",
    "    outpathCPU = outpath + \"cpu\" + os.path.sep\n",
    "\n",
    "    os.makedirs(outpathMem, exist_ok=True)\n",
    "    os.makedirs(outpathCPU, exist_ok=True)\n",
    "    minTime = 9999999999999999\n",
    "    for k in data:\n",
    "        cpu = data[k][\"cpu\"]\n",
    "        running = data[k][\"running\"]\n",
    "        minTime = min(min(running),minTime)\n",
    "        plt.plot(running, cpu, label = data[k][\"name\"][:40])\n",
    "    if(transition is not None):\n",
    "        for t in transition:\n",
    "            if t[1]/1000 > minTime:\n",
    "                plt.axvspan(t[1] / 1000, t[2]/1000, facecolor='r', alpha=0.2)    \n",
    "    plt.legend() \n",
    "    plt.ylabel(\"CPU usage in %\")\n",
    "    #plt.ylim((0,5))\n",
    "    plt.savefig(outpathCPU + server + \"_\" + str(run) + \".pdf\")\n",
    "    plt.savefig(outpathCPU + server + \"_\" + str(run) + \".jpg\", dpi = 300)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    for k in data:\n",
    "        mem = data[k][\"mem\"]\n",
    "        running = data[k][\"running\"]\n",
    "        plt.plot(running, mem, label = data[k][\"name\"][:40])\n",
    "    plt.ylabel(\"memory usage in %\")\n",
    "    plt.legend()   \n",
    "    #plt.ylim((0,5))\n",
    "    plt.savefig(outpathMem + server + \"_\" + str(run) + \".pdf\")\n",
    "    plt.savefig(outpathMem + server + \"_\" + str(run) + \".jpg\", dpi = 300)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805eadc67b7345998f4da8e09c1da571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    p = path + run + os.path.sep + \"logs\" + os.path.sep + \"usage\" + os.path.sep\n",
    "    servers = os.listdir(p)\n",
    "    for server in tqdm(servers):\n",
    "        file = open(p + server + os.path.sep + \"ps.log\", 'r') \n",
    "        lines = file.readlines() \n",
    "        file.close()\n",
    "        extractUsage(lines, server, run, transitions[run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsibilities\n",
    "\n",
    "shows already the server ID not kafka ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'33': [('initial', 1603631250000, 1603631273000),\n",
       "  ('s2', 1603631873000, 1603631893000),\n",
       "  ('s3', 1603632194000, 1603632213000),\n",
       "  ('initial', 1603632634000, 1603632648000)]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_33_t2',\n",
       " 'run_33_t1',\n",
       " 'run_33_t6',\n",
       " 'run_33_t5',\n",
       " 'run_33_t4',\n",
       " 'run_33_t3',\n",
       " 'run_33_t7']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
